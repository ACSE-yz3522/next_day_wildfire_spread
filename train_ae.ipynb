{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from tensorflow import estimator as tf_estimator\n",
    "import models.losses as losses\n",
    "import tensorflow as tf\n",
    "from models.metrics import *\n",
    "import models.cnn_autoencoder_model as cnnmodel\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # 数据路径\n",
    "    'train_path': '../dataset/next_day_wildfire_spread_train*',\n",
    "    'eval_path': '../dataset/next_day_wildfire_spread_eval*',\n",
    "    'test_path': '../dataset/next_day_wildfire_spread_test*',\n",
    "    \n",
    "    # 特征\n",
    "    'input_features': ['elevation', 'pdsi', 'NDVI', 'pr', 'sph', 'th', 'tmmn',\n",
    "                  'tmmx', 'vs', 'erc', 'population', 'PrevFireMask'],\n",
    "    'output_features': ['FireMask'],\n",
    "    \n",
    "    # 方位通道\n",
    "    'azimuth_in_channel': None,\n",
    "    'azimuth_out_channel': None,\n",
    "    \n",
    "    # 数据和模型参数\n",
    "    'data_sample_size': 64,\n",
    "    'sample_size': 32,\n",
    "    'output_sample_size': 32,\n",
    "    'batch_size': 128,\n",
    "    'shuffle': False,\n",
    "    'shuffle_buffer_size': 10000,\n",
    "    'compression_type': None,\n",
    "    'input_sequence_length': 1,\n",
    "    'output_sequence_length': 1,\n",
    "    'repeat': False,\n",
    "    'clip_and_normalize': True,\n",
    "    'clip_and_rescale': False,\n",
    "    \n",
    "    # 数据增强\n",
    "    'random_flip': False,\n",
    "    'random_rotate': False,\n",
    "    'random_crop': False,\n",
    "    'center_crop': True,\n",
    "    \n",
    "    # 其他参数\n",
    "    'downsample_threshold': 0.0,\n",
    "    'binarize_output': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.make_dataset(\n",
    "    hparams,\n",
    "    mode = tf_estimator.ModeKeys.TRAIN\n",
    ")\n",
    "val_dataset = dataset.make_dataset(\n",
    "    hparams,\n",
    "    mode = tf_estimator.ModeKeys.EVAL\n",
    ")\n",
    "test_dataset = dataset.make_dataset(\n",
    "    hparams,\n",
    "    mode = tf_estimator.ModeKeys.PREDICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 12)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   1728        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32, 32, 16)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   192         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 32, 32, 16)   0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2304        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 32, 32, 16)  0           ['dropout_1[0][0]',              \n",
      " da)                                                              'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 32, 32, 16)   0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 32, 16)   0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 16, 16, 16)   0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 16, 16, 16)   0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 16, 16, 16)   0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 32)   4608        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 32)   512         ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 16, 16, 32)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 16, 16, 32)  0           ['conv2d_4[0][0]',               \n",
      " mbda)                                                            'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 16, 16, 32)   0           ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 16, 16, 32)   0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 32)    0           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 8, 8, 32)     0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 8, 8, 32)     0           ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 8, 8, 32)     9216        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 8, 8, 32)     1024        ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 8, 8, 32)     0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 8, 8, 32)    0           ['conv2d_6[0][0]',               \n",
      " mbda)                                                            'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 16, 16, 32)   0           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16, 16, 32)   0           ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 16, 16, 32)   0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   9216        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 16, 16, 32)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 16, 16, 32)   0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9216        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   1024        ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 16, 16, 32)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 16, 16, 32)  0           ['conv2d_9[0][0]',               \n",
      " mbda)                                                            'dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 32)  0           ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 32, 32, 32)   0           ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 32, 32, 32)   0           ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   4608        ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 32, 32, 16)   0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 32, 32, 16)   0           ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2304        ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   512         ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 32, 32, 16)  0           ['conv2d_12[0][0]',              \n",
      " mbda)                                                            'dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 1)    16          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,480\n",
      "Trainable params: 46,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input((32, 32, 12))\n",
    "num_out_channels = 1\n",
    "encoder_layers = [16,32]\n",
    "decoder_layers = [32,16]\n",
    "encoder_pools = [2,2]\n",
    "decoder_pools = [2,2]\n",
    "autoencoder_model = cnnmodel.create_model(\n",
    "    input_tensor,\n",
    "    num_out_channels,\n",
    "    encoder_layers,\n",
    "    decoder_layers,\n",
    "    encoder_pools,\n",
    "    decoder_pools,\n",
    ")\n",
    "autoencoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "118/118 [==============================] - 21s 35ms/step - loss: 0.2836 - auc_with_masked_class: 0.7639 - val_loss: 0.2968 - val_auc_with_masked_class: 0.8104\n",
      "Epoch 2/1000\n",
      " 35/118 [=======>......................] - ETA: 8s - loss: 0.2271 - auc_with_masked_class: 0.8596"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\IRP\\next_day_wildfire_spread\\train_ae.ipynb 单元格 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IRP/next_day_wildfire_spread/train_ae.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IRP/next_day_wildfire_spread/train_ae.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m autoencoder_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IRP/next_day_wildfire_spread/train_ae.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               loss\u001b[39m=\u001b[39mlosses\u001b[39m.\u001b[39mweighted_cross_entropy_with_logits_with_masked_class(pos_weight\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IRP/next_day_wildfire_spread/train_ae.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[AUCWithMaskedClass(with_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IRP/next_day_wildfire_spread/train_ae.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m autoencoder_model\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mval_dataset, callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\IRP-tf-210\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "autoencoder_model.compile(optimizer=optimizer,\n",
    "              loss=losses.weighted_cross_entropy_with_logits_with_masked_class(pos_weight=3),\n",
    "              metrics=[AUCWithMaskedClass(with_logits=True)])\n",
    "history = autoencoder_model.fit(train_dataset, epochs=1000, validation_data=val_dataset, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 25ms/step - loss: 0.2032 - auc_with_masked_class: 0.3595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20317737758159637, 0.3595373034477234]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 12)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 32, 32, 16)  316         ['input_2[0][0]']                \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['separable_conv2d[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 32, 32, 16)  416         ['activation[0][0]']             \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   208         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 32, 32, 16)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 16)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 16, 16, 32)  688         ['max_pooling2d_2[0][0]']        \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 32)  128         ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 16, 16, 32)  1344        ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 32)  128         ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   544         ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_15[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 16, 16, 32)   0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 32)    0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 8, 8, 64)    2400        ['max_pooling2d_3[0][0]']        \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 8, 8, 64)    4736        ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8, 8, 64)    256         ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_16[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 8, 8, 64)     0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 8, 8, 64)     0           ['batch_normalization_8[0][0]',  \n",
      "                                                                  'dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 64)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 4, 4, 128)   8896        ['max_pooling2d_4[0][0]']        \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 4, 4, 128)   512         ['separable_conv2d_6[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 4, 4, 128)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 4, 4, 128)   17664       ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 4, 4, 128)   512         ['separable_conv2d_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 4, 4, 128)    8320        ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 4, 4, 128)    0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 4, 4, 128)   512         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4, 4, 128)    0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4, 4, 128)    0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 2, 2, 128)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 2, 2, 256)   34176       ['max_pooling2d_5[0][0]']        \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 2, 2, 256)   1024        ['separable_conv2d_8[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 2, 2, 256)    0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 2, 2, 256)   68096       ['activation_8[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 2, 2, 256)   1024        ['separable_conv2d_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 2, 2, 256)    33024       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 2, 2, 256)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 2, 2, 256)    0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 2, 2, 256)    0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 4, 4, 256)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 384)    0           ['up_sampling2d_2[0][0]',        \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 4, 4, 128)   52736       ['concatenate[0][0]']            \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 4, 4, 128)   512         ['separable_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 4, 4, 128)   17664       ['activation_10[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 4, 4, 128)   512         ['separable_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 4, 4, 128)    49280       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 4, 4, 128)   512         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4, 4, 128)    0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4, 4, 128)    0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 8, 8, 128)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 8, 8, 192)    0           ['up_sampling2d_3[0][0]',        \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 8, 8, 64)    14080       ['concatenate_1[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['separable_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 8, 8, 64)    4736        ['activation_12[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 64)    256         ['separable_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     12352       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 8, 8, 64)     0           ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['batch_normalization_20[0][0]', \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 64)  0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 16, 16, 96)   0           ['up_sampling2d_4[0][0]',        \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 16, 16, 32)  3968        ['concatenate_2[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 32)  128         ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 16, 16, 32)  1344        ['activation_14[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 32)  128         ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   3104        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 16, 16, 32)   0           ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_23[0][0]', \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 32)  0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 48)   0           ['up_sampling2d_5[0][0]',        \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 32, 32, 16)  1216        ['concatenate_3[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 16)  64          ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 32, 32, 16)  416         ['activation_16[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 16)  64          ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 32, 32, 16)   784         ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 32, 32, 16)  64          ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 32, 32, 16)   0           ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_26[0][0]', \n",
      "                                                                  'dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 32, 32, 1)    17          ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 32, 32, 1)   4           ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 353,473\n",
      "Trainable params: 349,055\n",
      "Non-trainable params: 4,418\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def expend_as(tensor, rep):\n",
    "     return Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                          arguments={'repnum': rep})(tensor)\n",
    "\n",
    "def double_conv_layer(x, filter_size, size, dropout, batch_norm=False):\n",
    "    axis = 3\n",
    "    conv = SeparableConv2D(size, (filter_size, filter_size), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = SeparableConv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    if dropout > 0:\n",
    "        conv = Dropout(dropout)(conv)\n",
    "\n",
    "    shortcut = Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        shortcut = BatchNormalization(axis=axis)(shortcut)\n",
    "\n",
    "    res_path = add([shortcut, conv])\n",
    "    return res_path\n",
    "\n",
    "def encoder(inputs):\n",
    "    num_filters = [16, 32, 64, 128]\n",
    "    skip_connections = []\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        a = double_conv_layer(x, 3, f, 0.1, True)\n",
    "        skip_connections.append(a)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(a)\n",
    "    \n",
    "    return x, skip_connections\n",
    "\n",
    "def bottleneck(inputs):\n",
    "    x = inputs\n",
    "    f = 256\n",
    "    \n",
    "    x3 = double_conv_layer(x, 3, f, 0.1, True)\n",
    "    \n",
    "    return x3\n",
    "\n",
    "def decoder(inputs, skip_connections):\n",
    "    num_filters = [128, 64, 32, 16]\n",
    "    skip_connections.reverse()\n",
    "    x = inputs\n",
    "    batch_norm = True\n",
    "    \n",
    "    for i, f in enumerate(num_filters):\n",
    "        \n",
    "        x_up = UpSampling2D(size=(2, 2), data_format=\"channels_last\")(x)\n",
    "        x_att = concatenate([x_up, skip_connections[i]], axis=-1)\n",
    "        \n",
    "        x = double_conv_layer(x_att, 3, f, 0.1, True)\n",
    "    return x\n",
    "\n",
    "def output(inputs):\n",
    "    x = Conv2D(1, kernel_size=(1,1))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = Activation('sigmoid')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "inputs = Input((32, 32, 12))\n",
    "# s = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)\n",
    "s = inputs\n",
    "x, skip_1 = encoder(s)\n",
    "x = bottleneck(x)\n",
    "x = decoder(x, skip_1)\n",
    "outputs = output(x)\n",
    "unet_model = Model(inputs, outputs)\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "118/118 [==============================] - 14s 59ms/step - loss: 0.6819 - auc_with_masked_class_1: 0.2517 - val_loss: 0.5865 - val_auc_with_masked_class_1: 0.0640\n",
      "Epoch 2/1000\n",
      "118/118 [==============================] - 11s 53ms/step - loss: 0.6078 - auc_with_masked_class_1: 0.3853 - val_loss: 0.5591 - val_auc_with_masked_class_1: 0.1325\n",
      "Epoch 3/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.5611 - auc_with_masked_class_1: 0.4154 - val_loss: 0.5305 - val_auc_with_masked_class_1: 0.2041\n",
      "Epoch 4/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.5200 - auc_with_masked_class_1: 0.4375 - val_loss: 0.5109 - val_auc_with_masked_class_1: 0.2637\n",
      "Epoch 5/1000\n",
      "118/118 [==============================] - 11s 52ms/step - loss: 0.4838 - auc_with_masked_class_1: 0.4532 - val_loss: 0.4818 - val_auc_with_masked_class_1: 0.3131\n",
      "Epoch 6/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.4507 - auc_with_masked_class_1: 0.4729 - val_loss: 0.4700 - val_auc_with_masked_class_1: 0.3127\n",
      "Epoch 7/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.4219 - auc_with_masked_class_1: 0.4842 - val_loss: 0.4348 - val_auc_with_masked_class_1: 0.3311\n",
      "Epoch 8/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.3960 - auc_with_masked_class_1: 0.4939 - val_loss: 0.4103 - val_auc_with_masked_class_1: 0.3463\n",
      "Epoch 9/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.3715 - auc_with_masked_class_1: 0.5097 - val_loss: 0.3991 - val_auc_with_masked_class_1: 0.3382\n",
      "Epoch 10/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.3499 - auc_with_masked_class_1: 0.5207 - val_loss: 0.3736 - val_auc_with_masked_class_1: 0.3483\n",
      "Epoch 11/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.3307 - auc_with_masked_class_1: 0.5302 - val_loss: 0.3591 - val_auc_with_masked_class_1: 0.3303\n",
      "Epoch 12/1000\n",
      "118/118 [==============================] - 11s 53ms/step - loss: 0.3123 - auc_with_masked_class_1: 0.5450 - val_loss: 0.3586 - val_auc_with_masked_class_1: 0.3239\n",
      "Epoch 13/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.2967 - auc_with_masked_class_1: 0.5503 - val_loss: 0.3350 - val_auc_with_masked_class_1: 0.3334\n",
      "Epoch 14/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.2825 - auc_with_masked_class_1: 0.5580 - val_loss: 0.3280 - val_auc_with_masked_class_1: 0.3311\n",
      "Epoch 15/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.2683 - auc_with_masked_class_1: 0.5730 - val_loss: 0.3192 - val_auc_with_masked_class_1: 0.3320\n",
      "Epoch 16/1000\n",
      "118/118 [==============================] - 11s 53ms/step - loss: 0.2573 - auc_with_masked_class_1: 0.5737 - val_loss: 0.3139 - val_auc_with_masked_class_1: 0.3274\n",
      "Epoch 17/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.2448 - auc_with_masked_class_1: 0.5920 - val_loss: 0.3132 - val_auc_with_masked_class_1: 0.3133\n",
      "Epoch 18/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.2346 - auc_with_masked_class_1: 0.5971 - val_loss: 0.2904 - val_auc_with_masked_class_1: 0.3402\n",
      "Epoch 19/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.2250 - auc_with_masked_class_1: 0.6066 - val_loss: 0.3010 - val_auc_with_masked_class_1: 0.3001\n",
      "Epoch 20/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.2244 - auc_with_masked_class_1: 0.5645 - val_loss: 0.2922 - val_auc_with_masked_class_1: 0.3130\n",
      "Epoch 21/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.2119 - auc_with_masked_class_1: 0.5997 - val_loss: 0.2867 - val_auc_with_masked_class_1: 0.3130\n",
      "Epoch 22/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.2059 - auc_with_masked_class_1: 0.5988 - val_loss: 0.2819 - val_auc_with_masked_class_1: 0.3108\n",
      "Epoch 23/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1962 - auc_with_masked_class_1: 0.6218 - val_loss: 0.2832 - val_auc_with_masked_class_1: 0.3047\n",
      "Epoch 24/1000\n",
      "118/118 [==============================] - 11s 54ms/step - loss: 0.1932 - auc_with_masked_class_1: 0.6082 - val_loss: 0.2710 - val_auc_with_masked_class_1: 0.3174\n",
      "Epoch 25/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1841 - auc_with_masked_class_1: 0.6343 - val_loss: 0.2710 - val_auc_with_masked_class_1: 0.3085\n",
      "Epoch 26/1000\n",
      "118/118 [==============================] - 12s 60ms/step - loss: 0.1792 - auc_with_masked_class_1: 0.6351 - val_loss: 0.2712 - val_auc_with_masked_class_1: 0.3010\n",
      "Epoch 27/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1732 - auc_with_masked_class_1: 0.6448 - val_loss: 0.2672 - val_auc_with_masked_class_1: 0.3095\n",
      "Epoch 28/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.1729 - auc_with_masked_class_1: 0.6244 - val_loss: 0.2645 - val_auc_with_masked_class_1: 0.3134\n",
      "Epoch 29/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1681 - auc_with_masked_class_1: 0.6320 - val_loss: 0.2642 - val_auc_with_masked_class_1: 0.3135\n",
      "Epoch 30/1000\n",
      "118/118 [==============================] - 12s 56ms/step - loss: 0.1595 - auc_with_masked_class_1: 0.6612 - val_loss: 0.2575 - val_auc_with_masked_class_1: 0.3274\n",
      "Epoch 31/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1545 - auc_with_masked_class_1: 0.6709 - val_loss: 0.2630 - val_auc_with_masked_class_1: 0.3123\n",
      "Epoch 32/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1528 - auc_with_masked_class_1: 0.6638 - val_loss: 0.2598 - val_auc_with_masked_class_1: 0.3186\n",
      "Epoch 33/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1480 - auc_with_masked_class_1: 0.6745 - val_loss: 0.2617 - val_auc_with_masked_class_1: 0.3205\n",
      "Epoch 34/1000\n",
      "118/118 [==============================] - 12s 61ms/step - loss: 0.1433 - auc_with_masked_class_1: 0.6860 - val_loss: 0.2593 - val_auc_with_masked_class_1: 0.3224\n",
      "Epoch 35/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1414 - auc_with_masked_class_1: 0.6826 - val_loss: 0.2673 - val_auc_with_masked_class_1: 0.2959\n",
      "Epoch 36/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1515 - auc_with_masked_class_1: 0.6143 - val_loss: 0.2567 - val_auc_with_masked_class_1: 0.3093\n",
      "Epoch 37/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1381 - auc_with_masked_class_1: 0.6788 - val_loss: 0.2619 - val_auc_with_masked_class_1: 0.3151\n",
      "Epoch 38/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1354 - auc_with_masked_class_1: 0.6819 - val_loss: 0.2590 - val_auc_with_masked_class_1: 0.3090\n",
      "Epoch 39/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1307 - auc_with_masked_class_1: 0.6963 - val_loss: 0.2655 - val_auc_with_masked_class_1: 0.3042\n",
      "Epoch 40/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1280 - auc_with_masked_class_1: 0.7023 - val_loss: 0.2572 - val_auc_with_masked_class_1: 0.3218\n",
      "Epoch 41/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.1253 - auc_with_masked_class_1: 0.7072 - val_loss: 0.2646 - val_auc_with_masked_class_1: 0.3110\n",
      "Epoch 42/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1242 - auc_with_masked_class_1: 0.7049 - val_loss: 0.2628 - val_auc_with_masked_class_1: 0.3035\n",
      "Epoch 43/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1338 - auc_with_masked_class_1: 0.6455 - val_loss: 0.2561 - val_auc_with_masked_class_1: 0.3190\n",
      "Epoch 44/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1224 - auc_with_masked_class_1: 0.7004 - val_loss: 0.2640 - val_auc_with_masked_class_1: 0.3070\n",
      "Epoch 45/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1242 - auc_with_masked_class_1: 0.6855 - val_loss: 0.2667 - val_auc_with_masked_class_1: 0.3142\n",
      "Epoch 46/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1268 - auc_with_masked_class_1: 0.6626 - val_loss: 0.2601 - val_auc_with_masked_class_1: 0.3151\n",
      "Epoch 47/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1191 - auc_with_masked_class_1: 0.6986 - val_loss: 0.2689 - val_auc_with_masked_class_1: 0.3007\n",
      "Epoch 48/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1203 - auc_with_masked_class_1: 0.6870 - val_loss: 0.2696 - val_auc_with_masked_class_1: 0.3172\n",
      "Epoch 49/1000\n",
      "118/118 [==============================] - 12s 59ms/step - loss: 0.1181 - auc_with_masked_class_1: 0.6948 - val_loss: 0.2662 - val_auc_with_masked_class_1: 0.3159\n",
      "Epoch 50/1000\n",
      "118/118 [==============================] - 12s 56ms/step - loss: 0.1128 - auc_with_masked_class_1: 0.7150 - val_loss: 0.2724 - val_auc_with_masked_class_1: 0.3091\n",
      "Epoch 51/1000\n",
      "118/118 [==============================] - 11s 55ms/step - loss: 0.1149 - auc_with_masked_class_1: 0.7015 - val_loss: 0.2682 - val_auc_with_masked_class_1: 0.3143\n",
      "Epoch 52/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.1100 - auc_with_masked_class_1: 0.7214 - val_loss: 0.2700 - val_auc_with_masked_class_1: 0.3117\n",
      "Epoch 53/1000\n",
      "118/118 [==============================] - 12s 60ms/step - loss: 0.1097 - auc_with_masked_class_1: 0.7178 - val_loss: 0.2733 - val_auc_with_masked_class_1: 0.3086\n",
      "Epoch 54/1000\n",
      "118/118 [==============================] - 12s 56ms/step - loss: 0.1078 - auc_with_masked_class_1: 0.7237 - val_loss: 0.2755 - val_auc_with_masked_class_1: 0.3103\n",
      "Epoch 55/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.1057 - auc_with_masked_class_1: 0.7300 - val_loss: 0.2786 - val_auc_with_masked_class_1: 0.3071\n",
      "Epoch 56/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1065 - auc_with_masked_class_1: 0.7233 - val_loss: 0.2730 - val_auc_with_masked_class_1: 0.3162\n",
      "Epoch 57/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1078 - auc_with_masked_class_1: 0.7146 - val_loss: 0.2832 - val_auc_with_masked_class_1: 0.3098\n",
      "Epoch 58/1000\n",
      "118/118 [==============================] - 12s 55ms/step - loss: 0.1041 - auc_with_masked_class_1: 0.7292 - val_loss: 0.2820 - val_auc_with_masked_class_1: 0.2979\n",
      "Epoch 59/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1049 - auc_with_masked_class_1: 0.7217 - val_loss: 0.2878 - val_auc_with_masked_class_1: 0.3100\n",
      "Epoch 60/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.1003 - auc_with_masked_class_1: 0.7414 - val_loss: 0.2843 - val_auc_with_masked_class_1: 0.3151\n",
      "Epoch 61/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.0996 - auc_with_masked_class_1: 0.7426 - val_loss: 0.2927 - val_auc_with_masked_class_1: 0.3015\n",
      "Epoch 62/1000\n",
      "118/118 [==============================] - 11s 57ms/step - loss: 0.1031 - auc_with_masked_class_1: 0.7246 - val_loss: 0.2743 - val_auc_with_masked_class_1: 0.3025\n",
      "Epoch 63/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1055 - auc_with_masked_class_1: 0.7109 - val_loss: 0.2803 - val_auc_with_masked_class_1: 0.3170\n",
      "Epoch 64/1000\n",
      "118/118 [==============================] - 12s 58ms/step - loss: 0.0987 - auc_with_masked_class_1: 0.7401 - val_loss: 0.2886 - val_auc_with_masked_class_1: 0.3093\n",
      "Epoch 65/1000\n",
      "118/118 [==============================] - 11s 56ms/step - loss: 0.0971 - auc_with_masked_class_1: 0.7448 - val_loss: 0.2920 - val_auc_with_masked_class_1: 0.3102\n",
      "Epoch 66/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.0998 - auc_with_masked_class_1: 0.7310 - val_loss: 0.2935 - val_auc_with_masked_class_1: 0.3111\n",
      "Epoch 67/1000\n",
      "118/118 [==============================] - 12s 56ms/step - loss: 0.0967 - auc_with_masked_class_1: 0.7435 - val_loss: 0.2940 - val_auc_with_masked_class_1: 0.3153\n",
      "Epoch 68/1000\n",
      "118/118 [==============================] - 12s 60ms/step - loss: 0.0960 - auc_with_masked_class_1: 0.7456 - val_loss: 0.2982 - val_auc_with_masked_class_1: 0.3132\n",
      "Epoch 69/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.1088 - auc_with_masked_class_1: 0.6857 - val_loss: 0.2829 - val_auc_with_masked_class_1: 0.3113\n",
      "Epoch 70/1000\n",
      "118/118 [==============================] - 12s 56ms/step - loss: 0.1043 - auc_with_masked_class_1: 0.7042 - val_loss: 0.2896 - val_auc_with_masked_class_1: 0.3158\n",
      "Epoch 71/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.0969 - auc_with_masked_class_1: 0.7366 - val_loss: 0.2965 - val_auc_with_masked_class_1: 0.3114\n",
      "Epoch 72/1000\n",
      "118/118 [==============================] - 12s 56ms/step - loss: 0.0950 - auc_with_masked_class_1: 0.7446 - val_loss: 0.2947 - val_auc_with_masked_class_1: 0.3073\n",
      "Epoch 73/1000\n",
      "118/118 [==============================] - 12s 57ms/step - loss: 0.0949 - auc_with_masked_class_1: 0.7432 - val_loss: 0.3056 - val_auc_with_masked_class_1: 0.3038\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "unet_model.compile(optimizer=optimizer,\n",
    "              loss=losses.weighted_cross_entropy_with_logits_with_masked_class(pos_weight=3),\n",
    "              metrics=[AUCWithMaskedClass(with_logits=True)])\n",
    "history = unet_model.fit(train_dataset, epochs=1000, validation_data=val_dataset, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2220 - auc_with_masked_class_1: 0.3633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2220226377248764, 0.36326098442077637]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRP-tf-210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
